import os
import ember
import numpy as np
import tensorflow as tf
from sklearn.decomposition import PCA
import sklearn

def init_vectorized_features(dataset_dir: str):
    """
    Required for the generation of '.dat' data files

    :param dataset_dir: directory to the base location of the dataset
    :return:
    """
    try:
        assert(os.path.exists(dataset_dir))

        ember.create_vectorized_features(dataset_dir, 1)
    except AssertionError:
        raise Exception(
            "[ASSERTION ERROR] The path to base directory of dataset provided does not exist"
        )


def dat_to_train_test(dat_dir: str):
    """
    Loading training & testing data from respective generated '.dat' files

    :param dat_dir: directory to the base location where generated '.dat' files are found
    :return:
    """
    try:
        assert('X_train.dat' in os.listdir(dat_dir))
        assert('y_train.dat' in os.listdir(dat_dir))
        assert ('X_test.dat' in os.listdir(dat_dir))
        assert ('y_test.dat' in os.listdir(dat_dir))

        x_train, y_train = ember.read_vectorized_features(dat_dir, subset="train")
        x_test, y_test = ember.read_vectorized_features(dat_dir, subset="test")

        return x_train, y_train, x_test, y_test
    except AssertionError:
        raise Exception(
            "[ASSERTION ERROR] Ensure that the required '.dat' files are found within the specified directory"
        )


def __dataset_generator(data: np.memmap, labels: np.memmap):
    """
    Helper function for conversion from numpy.memmap to tf.data.Dataset
    Create callable generator for tf.data.Dataset.from_generator()

    :param data: numpy.memmap of training data
    :param labels: numpy.memmap of labels corresponding to the training data
    :return:
    """
    # requires nothing to be passed to generator to avoid "TypeError: 'generator' must be callable." error
    def generator():
        for instance, label in zip(data, labels):
            yield instance, label
    return generator


def to_tf_dataset(x_memmap_data: np.memmap, y_memmap_data: np.memmap):
    """
    Convert numpy.memmap to tf.data.Dataset via the creation of generator with helper function '__dataset_generator()'

    :return:
    """
    return tf.data.Dataset.from_generator(__dataset_generator(x_memmap_data, y_memmap_data),
                                          output_types=(x_memmap_data.dtype, y_memmap_data.dtype),
                                          output_shapes=([x_memmap_data.shape[1], ], []))


def __unlabelled(data: tf.Tensor, label: tf.Tensor):
    """
    Helper function to act as a callable conditional statement for tf.data.Dataset.filter()

    Note: data is an unused parameter but is necessary for the proper functionality of this function
    (i.e. do not remove)

    :param data: tensor representation of data within the tf.data.Dataset
    :param label: tensor representation of label for the respective data within tf.data.Dataset
    :return:
    """
    if label == -1.0:
        return False
    return True


def rm_unlabelled_samples(dataset: tf.data.Dataset):
    """
    Filter all unlabelled data instances (label = -1.0) from the tf.data.Dataset passed in as parameter

    :param dataset: dataset in which the unlabelled instances (label = -1.0) are to be filtered out.
    :return:
    """
    return dataset.filter(__unlabelled)


def to_batch_dataset(dataset: tf.data.Dataset, batchsize: int = 100, drop_remainder: bool = False):
    """
    Function for converting from tf.data.Dataset type output by the `from_generator` function to a `BatchDataset`

    :param dataset: Tensorflow dataset generated from the use of `from_generator` Tensorflow function
    :param batchsize: The number of data records to be included in the batches for training
    :param drop_remainder: Boolean for determining whether or not data samples that dont fit in the specified batches
    should be dropped or not
    :return:
    """
    return dataset.batch(batchsize, drop_remainder)

def normalize_data(dataset: np.memmap):
    """

    :param dataset: The input dataset in numpy memmap format (Before conversion to Tensor)
    :return: Return the Robust scaled dataset
    """
    scaler = sklearn.preprocessing.RobustScaler()

    for idx, sample in enumerate(dataset):
        sample_reshape = sample.reshape(-1, 1)
        robust_norm = scaler.fit_transform(sample_reshape)
        robust_norm = robust_norm.reshape(1, -1)[0]
        dataset[idx] = robust_norm

    return dataset

def dataset_pca_reduction(train_dataset: np.memmap, test_dataset: np.memmap):
    """

    :param train_dataset: Numpy memmap containing feature vectors for training set
    :param test_dataset: Numpy memmap containing feature vectors for testing set
    :return: PCA reduction of the original training and testing datasets
    """

    pca = PCA(0.95)
    pca = pca.fit(train_dataset)
    print("Number of components kept by PCA: " + str(pca.n_components_))
    train_pca = pca.transform(train_dataset)
    test_pca = pca.transform(test_dataset)

    return train_pca, test_pca